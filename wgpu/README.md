# Compressed Matrix-Vector Multiplication in WebGPU (using the `wgpu` rust crate)

This directory contains a simple demo that demonstrates how inference in large language models (LLMs) can potentially be accelerated on consumer-grade GPUs by operating directly on a compressed representation of the network.
For more details, see the [technical motivation in the parent directory](../README.md#technical-details-and-distinction-from-prior-work).

The demo simulates the computational bottleneck of LLM inference by performing a sequence of matrix-vector multiplications (which is the type of operation that dominates LLM inference cost once the prompt has been parsed).
What's special about our setup here is that we load the matrices on the GPU _in compressed form_, i.e., not only quantized but also entropy coded (think "gzip" but better).
The GPU kernel (aka shader) decompresses the matrices on the fly while performing the matrix-vector multiplications in such a way that the GPU never has to hold the full uncompressed matrix in memory.
This reduces the amount of memory that the GPU has to access, and it turns out that this reduced memory access leads to a speedup despite the additional work that the GPU has to do for decompression (LLM inference is heavily _bandwidth bound_).

As an additional advantage, performing AI inference directly on the compressed representation of weight matrices allows fitting larger models on consumer-grade GPUs, which typically have relatively little GPU memory.

**Disclaimer:** this demo is an early proof of concept.
The tested workload is realistic for LLM inference, but both the proposed compute shader and the baseline can still be considerably improved.

## How to run

1. Install rust as described here: <https://rustup.rs/>
2. Clone this repository and `cd` into this directory
3. Create a file with several compressed matrices (by default: 100 matrices of shape 4096x4096, where the matrix entries are drawn i.i.d. from a quantized Gaussian distribution with standard deviation 4.0; see below for more options):

```bash
cargo run --bin mk-random --release -- -v testmatrices-compressed.bin
```

4. Run the series of matrix-vector multiplications on your GPU:

```bash
cargo run --bin demo --release -- testmatrices-compressed.bin
```

Here's what the program prints on my laptop (as of commit e68480e from March 25, 2025):

```text
Total number of matrix elements: 1.68e9
Reporting mean ± standard error over 10 runs (after discarding 2 runs of warmup) ...
Including upload:     duration = 506.0 ±  3.8 ms;  throughput =  3.32 ± 0.03 G elements/second
Not including upload: duration = 102.6 ±  0.3 ms;  throughput = 16.35 ± 0.04 G elements/second
```

And here's what I get for a simple baseline with uncompressed matrices (by providing the `--uncompressed` switch to both `mk-random` and `demo`):

```text
Total number of matrix elements: 1.68e9
Reporting mean ± standard error over 10 runs (after discarding 2 runs of warmup) ...
Including upload:     duration = 1168.6 ± 42.2 ms;  throughput =  1.45 ± 0.05 G elements/second
Not including upload: duration = 120.4 ±  2.7 ms;  throughput = 14.00 ± 0.32 G elements/second
```

So the compressed matrix-vector multiplication is faster on my hardware, especially if you count the time it takes to load the matrices on the GPU (which is relevant for models that don't fit entirely on a consumer GPU and thus have to be uploaded and executed in parts).
However, keep in mind that these are very early results, and the baseline is very naive (which also explains the low overall throughput).

## More Options

For more command line options, run `cargo run --bin mk-random --release -- --help` or `cargo run --bin demo --release -- --help`.

To verify correctness of both the encoder and decoder, use the command `verify-random` (get its help message with `cargo run --bin verify-random --release -- --help`).
This command reads compressed matrices from a file, decompresses them on the CPU, and verifies that the decompressed matrices are correct (by comparing to a ground truth generated by using the same random seed as `mk-random`).
You can also write out intermediate results of the matrix-vector multiplications to a `safetensors` file by using `--debug outfile.safetensors` with both `verify-random` and `demo`.
Load the safetensor files into a jupyter notebook to compare them.
